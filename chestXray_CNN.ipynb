{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fj8mQTiqXvW",
        "outputId": "0afcf069-c75e-4d22-ea04-610275c1ae75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chest-X-Ray-Dataset'...\n",
            "remote: Enumerating objects: 1027, done.\u001b[K\n",
            "remote: Total 1027 (delta 0), reused 0 (delta 0), pack-reused 1027 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1027/1027), 119.17 MiB | 25.07 MiB/s, done.\n",
            "Updating files: 100% (1026/1026), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Gurupatil0003/Chest-X-Ray-Dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2WpuS-jrU-g"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a_FKLAWwE2S"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE=64\n",
        "BATCH_SIZE = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU6z9O3KwUUD"
      },
      "outputs": [],
      "source": [
        "train_path =\"/content/Chest-X-Ray-Dataset/chest_xray/test\"\n",
        "test_path =\"/content/Chest-X-Ray-Dataset/chest_xray/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWzGlKGSxNPE",
        "outputId": "a31fa7d7-b87f-4f54-afbf-bdf37ab4ea61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 333 images belonging to 2 classes.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.legacy.preprocessing.image.DirectoryIterator at 0x79019e01ead0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#image preprocessing using ImageDataGenerator for training data\n",
        "train_data = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(IMG_SIZE,IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary' #classification type\n",
        "    )\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF6K6m8k-yBd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6cbP3EHxEnL",
        "outputId": "984e88c6-8003-4e17-e505-a2af7daf580c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 333 images belonging to 2 classes.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.legacy.preprocessing.image.DirectoryIterator at 0x79019dd8a7d0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#image preprocessing using ImageDataGenerator for testing data\n",
        "test_data = ImageDataGenerator(rescale=1/255).flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(IMG_SIZE,IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary' #classification type\n",
        "    )\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnlLi30n0TFd",
        "outputId": "a6f4f642-5bd4-421c-fa63-f3bd8c4cc914"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,3)), #convolutional layer with 32 neurons\n",
        "    MaxPooling2D((2,2)),                            #pooling layer\n",
        "    Conv2D(64,(3,3),activation='relu'),        #convolutional layer with 64 neurons\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64,(3,3),activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128,activation='relu'),\n",
        "    Dense(128,activation='relu'),#fully connected layers layers with 128 neurons\n",
        "    Dense(1,activation=\"sigmoid\")               #output layer -only 1 neuron is needed if there is only 1 output(yes/NO) and sigmoid function because of binary classification\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeW5Dejy7TX8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54aRkSHN72cy",
        "outputId": "7e51851a-9505-4d9f-94a3-e8d19fc30b91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 977ms/step - accuracy: 0.4712 - loss: 0.7010 - val_accuracy: 0.5946 - val_loss: 0.6824\n",
            "Epoch 2/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 906ms/step - accuracy: 0.5863 - loss: 0.6806 - val_accuracy: 0.5946 - val_loss: 0.6558\n",
            "Epoch 3/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 668ms/step - accuracy: 0.6043 - loss: 0.6454 - val_accuracy: 0.8228 - val_loss: 0.5821\n",
            "Epoch 4/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 796ms/step - accuracy: 0.7710 - loss: 0.5633 - val_accuracy: 0.7898 - val_loss: 0.4651\n",
            "Epoch 5/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 715ms/step - accuracy: 0.8218 - loss: 0.4085 - val_accuracy: 0.8078 - val_loss: 0.4255\n",
            "Epoch 6/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 738ms/step - accuracy: 0.8587 - loss: 0.3865 - val_accuracy: 0.8889 - val_loss: 0.2774\n",
            "Epoch 7/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 793ms/step - accuracy: 0.8869 - loss: 0.3074 - val_accuracy: 0.9129 - val_loss: 0.2636\n",
            "Epoch 8/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 776ms/step - accuracy: 0.8664 - loss: 0.3185 - val_accuracy: 0.8859 - val_loss: 0.2603\n",
            "Epoch 9/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 675ms/step - accuracy: 0.9060 - loss: 0.2539 - val_accuracy: 0.9219 - val_loss: 0.2263\n",
            "Epoch 10/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 780ms/step - accuracy: 0.9309 - loss: 0.1986 - val_accuracy: 0.9369 - val_loss: 0.1942\n",
            "Epoch 11/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 880ms/step - accuracy: 0.9111 - loss: 0.2292 - val_accuracy: 0.9369 - val_loss: 0.2009\n",
            "Epoch 12/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 777ms/step - accuracy: 0.9264 - loss: 0.2069 - val_accuracy: 0.9369 - val_loss: 0.1789\n",
            "Epoch 13/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 789ms/step - accuracy: 0.9400 - loss: 0.1564 - val_accuracy: 0.9279 - val_loss: 0.1655\n",
            "Epoch 14/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 873ms/step - accuracy: 0.9438 - loss: 0.1705 - val_accuracy: 0.9459 - val_loss: 0.1401\n",
            "Epoch 15/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 674ms/step - accuracy: 0.9307 - loss: 0.1645 - val_accuracy: 0.9459 - val_loss: 0.1655\n",
            "Epoch 16/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 785ms/step - accuracy: 0.9459 - loss: 0.1440 - val_accuracy: 0.9640 - val_loss: 0.1202\n",
            "Epoch 17/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 882ms/step - accuracy: 0.9470 - loss: 0.1725 - val_accuracy: 0.9610 - val_loss: 0.1224\n",
            "Epoch 18/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 674ms/step - accuracy: 0.9572 - loss: 0.1188 - val_accuracy: 0.9670 - val_loss: 0.1107\n",
            "Epoch 19/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 794ms/step - accuracy: 0.9740 - loss: 0.0795 - val_accuracy: 0.9700 - val_loss: 0.0925\n",
            "Epoch 20/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 872ms/step - accuracy: 0.9684 - loss: 0.1080 - val_accuracy: 0.9790 - val_loss: 0.0653\n",
            "Epoch 21/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 684ms/step - accuracy: 0.9815 - loss: 0.0678 - val_accuracy: 0.9790 - val_loss: 0.0747\n",
            "Epoch 22/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 787ms/step - accuracy: 0.9919 - loss: 0.0578 - val_accuracy: 0.9940 - val_loss: 0.0399\n",
            "Epoch 23/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 872ms/step - accuracy: 0.9834 - loss: 0.0366 - val_accuracy: 0.9730 - val_loss: 0.0696\n",
            "Epoch 24/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 675ms/step - accuracy: 0.9921 - loss: 0.0461 - val_accuracy: 0.9670 - val_loss: 0.0639\n",
            "Epoch 25/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 792ms/step - accuracy: 0.9718 - loss: 0.0677 - val_accuracy: 0.9550 - val_loss: 0.0930\n",
            "Epoch 26/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 881ms/step - accuracy: 0.9845 - loss: 0.0639 - val_accuracy: 0.9730 - val_loss: 0.0700\n",
            "Epoch 27/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 674ms/step - accuracy: 0.9846 - loss: 0.0567 - val_accuracy: 0.9940 - val_loss: 0.0267\n",
            "Epoch 28/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 863ms/step - accuracy: 0.9834 - loss: 0.0509 - val_accuracy: 0.9970 - val_loss: 0.0239\n",
            "Epoch 29/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 774ms/step - accuracy: 0.9890 - loss: 0.0265 - val_accuracy: 0.9940 - val_loss: 0.0332\n",
            "Epoch 30/30\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 673ms/step - accuracy: 0.9973 - loss: 0.0259 - val_accuracy: 0.9970 - val_loss: 0.0183\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79019f926f50>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_data,epochs=30,validation_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfMDSTLK-TzC",
        "outputId": "9362ea7f-47d7-4f32-e292-f68a0fc6d823"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"brain_tumour_model.h5\") #saving the model to h5 file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KinYgC0_0jL"
      },
      "source": [
        "## \\Creating a UI interface for testing our model using **Gradio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnwxXqew_QUm"
      },
      "outputs": [],
      "source": [
        "#pip install gradio #to create UI interface for CNN model we use gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXJJFMLr_XIW"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUdbuRNqAN7Z"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 64\n",
        "class_name = [\"Normal\",\"pneumonia\"]\n",
        "\n",
        "def predict(img):\n",
        "  img = img.resize((IMG_SIZE,IMG_SIZE))\n",
        "  img = np.array(img)/255.0\n",
        "  img = np.expand_dims(img,axis=0)\n",
        "  prediction = model.predict(img)[0][0]\n",
        "  return class_name[int(prediction[0][0])]\n",
        "\n",
        "# def predict(img):\n",
        "#   #image = img.reshape(1,IMG_SIZE,IMG_SIZE,3) reshaping the image,here 1 means batch size (batch_size=1,only image is considered)\n",
        "#   #rescaling the the image\n",
        "#   image = img.rescale(IMG_SIZE,IMG_SIZE)\n",
        "#   # Convert the image to a NumPy array and scale pixel values to be between 0 and 1\n",
        "#   image_array = np.array(image)/255.0\n",
        "#   # Add a batch dimension to the array\n",
        "#   image_array = np.expand_dims(image_array,axis=0)\n",
        "#   # Get the prediction from the model\n",
        "#   prediction = model.predict(image_array)[0][0]\n",
        "#   # Determine the predicted label based on the prediction probability\n",
        "  # if prediction > 0.5: # Assuming a threshold of 0.5 for binary classification\n",
        "  #     label = class_name[1] # Pneumonia\n",
        "  # else:\n",
        "  #     label = class_name[0] # Normal\n",
        "\n",
        "  # # Return the predicted label and the prediction probability\n",
        "  # return label, float(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UrOImegfgGX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "gp6ELzJ5rsUH",
        "outputId": "3986dada-11a7-42b7-ea18-79aef3a0b085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://26a36ebce5f3916953.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://26a36ebce5f3916953.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "interface=gr.Interface(\n",
        "    fn = predict,     # when ever u want to write function use fn.\n",
        "    inputs=gr.Image(type = 'pil'),\n",
        "    outputs=gr.Textbox(label='Prediction')\n",
        ")\n",
        "interface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}